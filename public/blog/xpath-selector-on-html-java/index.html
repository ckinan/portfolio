<!DOCTYPE html>
<html>
<head>
    <title>ckinan.com: XPath selector on HTML with Java</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="/styles.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="/about">About</a></li>
            </ul>
        </nav>
    </header>
    <h1>XPath selector on HTML with Java</h1>
    <section>
        <p>
    Date: 2020-09-30
</p>
<p>I wanted to explore how easy is to extract elements from HTML files or directly from website URLs programatically using Java. I found a third party library that gave me the desired results without much difficulties.</p>
<p>It's XSoup.</p>
<ul>
<li>Github Repo: https://github.com/code4craft/xsoup</li>
<li>Maven Repo: https://mvnrepository.com/artifact/us.codecraft/xsoup</li>
</ul>
<p>With XPath expressions it is able to select the elements within the HTML using <a href="https://jsoup.org/">Jsoup</a> as HTML parser.</p>
<p>In the example below, it extracts the posts of this website using the URL (https://ckinan.com/blog) , or reading the html of that exact url but downloaded into the repository.</p>
<h2>The code</h2>
<p>Repo: https://github.com/ckinan/java-practice/tree/master/parse-html</p>
<p>We got:</p>
<ul>
<li>One single file <code>App.java</code> to place our logic</li>
<li>One single dependency in our <code>pom.xml</code> file: XSoup</li>
</ul>
<h3>App.java</h3>
<p>I tried to explain the code with the docstrings and comments there to point out what the lines and methods are supposed to do.</p>
<pre><code class="language-java">package com.ckinan;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import us.codecraft.xsoup.Xsoup;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import java.util.stream.Collectors;

/**
 * Code snippet to find elements within a html (from file and url) using XPath.
 *
 * It uses a third party library called XSoup. Links:
 * - mvn repository: https://mvnrepository.com/artifact/us.codecraft/xsoup
 * - github repository: https://github.com/code4craft/xsoup
 *
 * This example reads a list of blog posts using a fixed XPath expression and prints the results in the console.
 * */
public class App {

    public static final String XPATH_BLOG_POST = &quot;//div[contains(@class, 'leading-snug')]&quot;;

    /**
     * The method main of the program. It starts and orchestrates the process.
     * It gets instances of Documents from specific sources and sends them to read specific elements
     * by xpath expression.
     *
     * It retrieve documents from:
     * - Files (by specifying the full path)
     * - URL (of the website to be parsed)
     *
     * @param  args
     *         Arguments of our application. Won't be used at all

     * @throws  java.io.IOException
     */
    public static void main(String[] args) throws IOException {
        // Calling the static methods that will get an instance of the HTML document and then evaluate
        // the XPath expression to get a list of Blog Posts. First with a HTML file, then do the same with the website
        // URL directly.
        Document doc1 = App.getDocumentFromFile(&quot;src/main/resources/blog.html&quot;);
        List&lt;String&gt; postsFromFile = App.readDocument(doc1, App.XPATH_BLOG_POST);
        System.out.println(postsFromFile);

        Document doc2 = App.getDocumentFromUrl(&quot;https://ckinan.com/blog&quot;);
        List&lt;String&gt; postsFromUrl = App.readDocument(doc2, App.XPATH_BLOG_POST);
        System.out.println(postsFromUrl);
    }

    /**
     * Read the content file of the given full path using &quot;collect&quot; from Stream Java 8+.
     *
     * Refs:
     * - https://docs.oracle.com/javase/8/docs/api/java/nio/file/Paths.html#get-java.lang.String-java.lang.String...-
     * - https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#lines-java.nio.file.Path-
     * - https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#collect-java.util.stream.Collector-
     *
     * Then it uses Jsoup to parse the content file into a Document instance.
     *
     * @param  fullPath
     *         The full path of the file to be read and parsed using Jsoup.

     * @throws  java.io.IOException
     *
     * @return  The parsed document which contains all the elements of the HTML file
     */
    public static Document getDocumentFromFile(String fullPath) throws IOException {
        System.out.println(&quot;Reading document from file: &quot; + fullPath);
        // Read the content file using Java 8+ Streams.
        String html = Files.lines(Paths.get(fullPath)).collect(Collectors.joining(System.lineSeparator()));
        return Jsoup.parse(html);
    }

    /**
     * Read the content of the given website url using Jsoup
     *
     * @param  url
     *         The URL of the website HTML to be read and parsed using Jsoup.
     *
     * @throws  java.io.IOException
     *
     * @return  The parsed document which contains all the elements of the HTML file
     */
    public static Document getDocumentFromUrl(String url) throws IOException {
        System.out.println(&quot;Reading document from url: &quot; + url);
        return Jsoup.connect(url).get();
    }

    /**
     * Evaluates the given XPath expression on the given Document which is limited to assume that it's a list of
     * Elements right now. Returns results of the XPath evaluation.
     *
     * @param  doc
     *         The URL of the website HTML to be read and parsed using Jsoup.
     *
     * @param  xpath
     *         The URL of the website HTML to be read and parsed using Jsoup.
     *
     * @throws  java.io.IOException
     *
     * @return  The list of string values resulted from the XPath expression.
     */
    public static List&lt;String&gt; readDocument(Document doc, String xpath) {
        List&lt;String&gt; result = new ArrayList&lt;&gt;();

        // It first extract the Element instances of the html content using the given Document and the XPath expression
        // Note: Xsoup uses Jsoup as the HTML parser. Xsoup &quot;evaluates&quot; a Document which is an instance variable created
        //       by Jsoup
        List&lt;Element&gt; elements =
                Xsoup.compile(xpath).evaluate(doc).getElements();

        for(Element e: elements) {
            // Each Element object has multiple properties available to be extracted, for example the HTML tag name.
            // For this code snipped, we are interested into extract the text of the element.
            // Example: &lt;p&gt;Hello world&lt;/p&gt; -&gt; e.text() would return &quot;Hello world&quot;
            result.add(e.text());
        }

        return result;
    }

}

</code></pre>
<h3>pom.xml (dependences section only)</h3>
<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;!-- https://mvnrepository.com/artifact/us.codecraft/xsoup --&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;us.codecraft&lt;/groupId&gt;
    &lt;artifactId&gt;xsoup&lt;/artifactId&gt;
    &lt;version&gt;0.3.1&lt;/version&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
<h2>Testing</h2>
<p>Run the <code>App.java</code> class, and it should get this printed in the Console:</p>
<pre><code>Reading document from file: src/main/resources/blog.html
[Sat 22:Remote Debug Spring Boot App in a Docker Container using IntelliJ IDEA, Thu 13:Cheat Sheet - Git Commands, Mon 20:Spring Security - Credentials from JSON request, Fri 03:Spring Security - Filter Chain, Sun 28:Spring Security - Form Login &amp; Logout, Wed 17:KnockoutJS - Register Component, Mon 01:Spring Security - Getting Started, Sun 17:Tailwind CSS - First Steps, Fri 24:Project Github Pull Request Viewer, Fri 13:Github API - OAuth tokens for apps, Sat 07:Netlify Functions, Fri 07:Github API - First Steps, Sun 05:Auth0 - Simple Example with Vanilla JS, Tue 08:Intro]
Reading document from url: https://ckinan.com/blog
[Sat 22:Remote Debug Spring Boot App in a Docker Container using IntelliJ IDEA, Thu 13:Cheat Sheet - Git Commands, Mon 20:Spring Security - Credentials from JSON request, Fri 03:Spring Security - Filter Chain, Sun 28:Spring Security - Form Login &amp; Logout, Wed 17:KnockoutJS - Register Component, Mon 01:Spring Security - Getting Started, Sun 17:Tailwind CSS - First Steps, Fri 24:Project Github Pull Request Viewer, Fri 13:Github API - OAuth tokens for apps, Sat 07:Netlify Functions, Fri 07:Github API - First Steps, Sun 05:Auth0 - Simple Example with Vanilla JS, Tue 08:Intro]
</code></pre>
<h2>Tooling</h2>
<p>Something uselful to &quot;test&quot; the XPath expression to see if it's a valid one or if will work, it is this chrome extension: <a href="https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl">XPath Helper</a>. For the above example I introduced the XPath expression <code>//div[contains(@class, 'leading-snug')]</code> and ensure the results made sense.</p>
<p><img src="./images/0.png" alt="" /></p>
<h2>Final thoughts</h2>
<p>There might be another way to accomplish the same thing I did with Xsoup, the github repo seems not to be pretty active lately (last commit was on Jan-2016 - at least this is what Github tells me).</p>
<p>But, I needed an XPath selector, and Xsoup worked well and without a lot of boilerplate, just like what I was looking for. So thanks Xsoup!</p>
<h2>Links:</h2>
<ul>
<li>XSoup: https://github.com/code4craft/xsoup</li>
<li>Jsoup: https://jsoup.org/</li>
<li>Repo of the code snippet: https://github.com/ckinan/java-practice/tree/master/parse-html</li>
<li>This tutorial helped me to understand the basics of XPath: https://www.w3schools.com/xml/xpath_syntax.asp</li>
</ul>

    </section>
</body>
</html>